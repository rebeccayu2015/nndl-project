"""
train_baseline.py
------------------

Training script for the baseline CNN model defined in baseline_cnn.py using
the project’s custom Dataset and preprocessing pipeline

data → transform → dataset → dataloader → model

Steps:
    1. Load the train/val split CSVs generated by data_utils.py
    2. Construct Dataset and DataLoader objects with the appropriate
       transforms/augmentation
    3. Initializes the baseline CNN model with two classification heads:
           - superclass prediction (3 classes)
           - subclass prediction (87 classes)
    4. Define the loss functions and optimizer (Adam in this case)
    5. Run the full training loop: forward pass, compute losses for both heads, backward pass + optimizer step, periodic validation
    6. Track accuracy and loss metrics each epoch
    7. Save: the best-performing model checkpoint, a metrics JSON file for later comparison, confusion matrices or sample predictions
"""

import os
import json
from typing import Dict
import numpy as np
import pandas as pd

import torch
from torch.utils.data import DataLoader

from src.utils.seed import set_seed
from src.data.dataset import BirdDogReptileDataset
from src.data.transforms import get_train_transform, get_eval_transform
from src.models.baseline_cnn import BaselineCNN
from src.training.eval import cross_entropy_loss, compute_super_sub_accuracies
from src.visualization.plots import plot_loss_curves
from src.visualization.cm import plot_confusion_matrix

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

def train_epoch(model, loader, optimizer, lambda_sub: float = 1.0):
    model.train()
    total_loss = 0.0
    total_super_acc = 0.0
    total_sub_acc = 0.0
    n_batches = 0

    for images, y_super, y_sub, _ in loader:
        images = images.to(DEVICE)
        y_super = y_super.to(DEVICE)
        y_sub = y_sub.to(DEVICE)

        optimizer.zero_grad()
        logits_super, logits_sub = model(images)
        loss = cross_entropy_loss(logits_super, logits_sub, y_super, y_sub, lambda_sub)
        loss.backward()
        optimizer.step()

        metrics = compute_super_sub_accuracies(logits_super.detach(), logits_sub.detach(), y_super, y_sub)
        total_loss += loss.item()
        total_super_acc += metrics["super_accuracy"]
        total_sub_acc += metrics["sub_accuracy"]
        n_batches += 1

    return {
        "loss": total_loss / n_batches,
        "super_accuracy": total_super_acc / n_batches,
        "sub_accuracy": total_sub_acc / n_batches,
    }

def eval_epoch(model, loader, lambda_sub: float = 1.0):
    model.eval()
    total_loss = 0.0
    total_super_acc = 0.0
    total_sub_acc = 0.0
    n_batches = 0

    with torch.no_grad():
        for images, y_super, y_sub, _ in loader:
            images = images.to(DEVICE)
            y_super = y_super.to(DEVICE)
            y_sub = y_sub.to(DEVICE)

            logits_super, logits_sub = model(images)
            loss = cross_entropy_loss(logits_super, logits_sub, y_super, y_sub, lambda_sub)

            metrics = compute_super_sub_accuracies(logits_super, logits_sub, y_super, y_sub)
            total_loss += loss.item()
            total_super_acc += metrics["super_accuracy"]
            total_sub_acc += metrics["sub_accuracy"]
            n_batches += 1

    return {
        "loss": total_loss / n_batches,
        "super_accuracy": total_super_acc / n_batches,
        "sub_accuracy": total_sub_acc / n_batches,
    }

def main():
    set_seed(42)

    # Paths
    train_csv = "data/splits/train_split.csv"
    val_csv = "data/splits/val_split.csv"
    images_root = "data/raw/train_images"
    superclass_map = "data/meta/superclass_mapping.csv"
    subclass_map = "data/meta/subclass_mapping.csv"

    batch_size = 64
    num_epochs = 25
    lambda_sub = 1.0
    lr = 1e-3

    num_superclasses = 3
    num_subclasses = 87

    train_dataset = BirdDogReptileDataset(
        csv_path=train_csv,
        images_root=images_root,
        transform=get_train_transform(),
        superclass_mapping_path=superclass_map,
        subclass_mapping_path=subclass_map,
    )

    val_dataset = BirdDogReptileDataset(
        csv_path=val_csv,
        images_root=images_root,
        transform=get_eval_transform(),
        superclass_mapping_path=superclass_map,
        subclass_mapping_path=subclass_map,
    )

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)

    model = BaselineCNN(num_superclasses=num_superclasses, num_subclasses=num_subclasses).to(DEVICE)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    os.makedirs("experiments/baseline/checkpoints", exist_ok=True)
    os.makedirs("experiments/baseline/logs", exist_ok=True)

    best_val_loss = float("inf")
    history = {"train": [], "val": []}

    for epoch in range(1, num_epochs + 1):
        train_stats = train_epoch(model, train_loader, optimizer, lambda_sub)
        val_stats = eval_epoch(model, val_loader, lambda_sub)

        print(f"[Epoch {epoch}] "
              f"train_loss={train_stats['loss']:.4f}, "
              f"val_loss={val_stats['loss']:.4f}, "
              f"val_super_acc={val_stats['super_accuracy']:.4f}, "
              f"val_sub_acc={val_stats['sub_accuracy']:.4f}")

        history["train"].append(train_stats)
        history["val"].append(val_stats)

        # Checkpoint best model
        if val_stats["loss"] < best_val_loss:
            best_val_loss = val_stats["loss"]
            ckpt_path = f"experiments/baseline/checkpoints/best_baseline.pt"
            torch.save({"model_state_dict": model.state_dict(),
                        "epoch": epoch,
                        "val_loss": best_val_loss},
                       ckpt_path)

    # Validation loop ot save true and predicted values (for confusion matrix generation)
    ckpt_path = "experiments/baseline/checkpoints/best_baseline.pt"
    ckpt = torch.load(ckpt_path, map_location=DEVICE)
    model.load_state_dict(ckpt["model_state_dict"])
    model.to(DEVICE)
    model.eval()

    all_super_true = []
    all_super_pred = []
    all_sub_true = []
    all_sub_pred = []

    with torch.no_grad():
        for images, y_super, y_sub, _ in val_loader:
            images = images.to(DEVICE)
            y_super = y_super.to(DEVICE)
            y_sub = y_sub.to(DEVICE)

            logits_super, logits_sub = model(images)
            preds_super = torch.argmax(logits_super, dim=1).cpu().numpy()
            preds_sub   = torch.argmax(logits_sub, dim=1).cpu().numpy()

            all_super_true.append(y_super.cpu().numpy())
            all_super_pred.append(preds_super)
            all_sub_true.append(y_sub.cpu().numpy())
            all_sub_pred.append(preds_sub)

    all_super_true = np.concatenate(all_super_true)
    all_super_pred = np.concatenate(all_super_pred)
    all_sub_true   = np.concatenate(all_sub_true)
    all_sub_pred   = np.concatenate(all_sub_pred)

    # Save arrays for later reuse (OSR, more plots, etc.)
    os.makedirs("experiments/baseline", exist_ok=True)
    np.save("experiments/baseline/val_labels_super.npy", all_super_true)
    np.save("experiments/baseline/val_preds_super.npy", all_super_pred)
    np.save("experiments/baseline/val_labels_sub.npy", all_sub_true)
    np.save("experiments/baseline/val_preds_sub.npy", all_sub_pred)

    # Generate confusion matrices
    try:
        super_df = pd.read_csv(superclass_map)
        super_names = super_df.iloc[:, 1].tolist()  # second column as names
    except Exception:
        super_names = ["bird", "dog", "reptile"]

    # Subclass names
    try:
        sub_df = pd.read_csv(subclass_map)
        subclass_names = sub_df.iloc[:, 1].tolist()
    except Exception:
        subclass_names = [str(i) for i in range(int(all_sub_true.max()) + 1)]

    plot_confusion_matrix(
        y_true=all_super_true,
        y_pred=all_super_pred,
        class_names=super_names,
        title="Baseline Superclass Confusion Matrix",
        save_path="experiments/baseline/confusion_super.png",
    )

    plot_confusion_matrix(
        y_true=all_sub_true,
        y_pred=all_sub_pred,
        class_names=subclass_names,
        title="Baseline Subclass Confusion Matrix",
        save_path="experiments/baseline/confusion_sub.png",
    )

    # Plot loss curves
    train_losses = [e["loss"] for e in history["train"]]
    val_losses   = [e["loss"] for e in history["val"]]

    plot_loss_curves(
        train_losses,
        val_losses,
        save_path="experiments/baseline/loss_curve.png",
    )

    # Save metrics
    with open("experiments/baseline/metrics.json", "w") as f:
        json.dump(history, f, indent=2)

if __name__ == "__main__":
    main()
